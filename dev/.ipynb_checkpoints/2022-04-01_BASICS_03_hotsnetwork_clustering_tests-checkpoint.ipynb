{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146e9eb1-7f15-4122-a866-cd3061b73da0",
   "metadata": {},
   "source": [
    "# BASICS 03 - HOTS in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a75ff8-780a-400b-9925-515f4a2274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6346880-ad3c-4a2c-a39a-1a8842c3ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antoine/homhots/hotsline/hots\n",
      "Tonic version installed -> 1.0.15\n",
      "Number of GPU devices available: 1\n",
      "GPU 1 named GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "%cd ../hots\n",
    "import tonic, torch, os\n",
    "from timesurface import timesurface\n",
    "from network import network\n",
    "from utils import get_loader\n",
    "\n",
    "print(f'Tonic version installed -> {tonic.__version__}')\n",
    "\n",
    "print(f'Number of GPU devices available: {torch.cuda.device_count()}')\n",
    "for N_gpu in range(torch.cuda.device_count()):\n",
    "    print(f'GPU {N_gpu+1} named {torch.cuda.get_device_name(N_gpu)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce771bc-ad2d-45c4-ba80-0a28862f07fe",
   "metadata": {},
   "source": [
    "## Initialization of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982b781c-f37b-4ee2-b811-b3b6c8be7eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in the dataset: 200\n"
     ]
    }
   ],
   "source": [
    "trainset = True\n",
    "transform = tonic.transforms.NumpyAsType(int)\n",
    "dataset = tonic.datasets.NMNIST(save_to='../../Data/', train=trainset, transform=transform)\n",
    "loader = get_loader(dataset, kfold = 300, shuffle=True)\n",
    "#get_dataset_info(dataset, properties = ['time', 'mean_isi', 'nb_events']);\n",
    "print(f'number of samples in the dataset: {len(loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1caf366e-fc63-4030-868b-a57babe598de",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'homeohots'\n",
    "homeo = True\n",
    "timestr = '2022-04-01'\n",
    "dataset_name = 'nmnist'\n",
    "\n",
    "Rz = (2, 4, 8)\n",
    "N_neuronz = (4, 8, 16)\n",
    "tauz = (5e3, 5e4, 5e5)\n",
    "\n",
    "network = network(name, dataset_name, timestr, dataset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804365ce-3c09-40a8-90ad-f2471961b169",
   "metadata": {},
   "source": [
    "##Â Unsupervised clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7064b789-7876-4611-8d34-3a5a4d62e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../Records/'):\n",
    "    os.mkdir('../Records/')\n",
    "    os.mkdir('../Records/networks/')\n",
    "filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "path = '../Records/networks/'+network.name+'.pkl'\n",
    "if not os.path.exists(path):\n",
    "    network.clustering(loader, dataset.ordering, filtering_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0361ae-8651-4ca6-95db-d3161988f90c",
   "metadata": {},
   "source": [
    "## Coding of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafb38e7-28d4-4efc-8375-7cb1a8fae24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this dataset have already been processed, check at: \n",
      " ../Records/output/train/2022-04-01_nmnist_homeohots_True_(4, 8, 16)_(5000.0, 50000.0, 500000.0)_(2, 4, 8)_60000_(None, None)/\n"
     ]
    }
   ],
   "source": [
    "loader = get_loader(dataset, kfold = None, shuffle=True)\n",
    "network.coding(loader, dataset.ordering, dataset.classes, filtering_threshold, training=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3ff6e-d596-4889-b65b-ebbd60eb23c3",
   "metadata": {},
   "source": [
    "## Coding of the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60152ea5-3bb9-45b2-ab2a-5cff12e353db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this dataset have already been processed, check at: \n",
      " ../Records/output/test/2022-04-01_nmnist_homeohots_True_(4, 8, 16)_(5000.0, 50000.0, 500000.0)_(2, 4, 8)_10000_(None, None)/\n"
     ]
    }
   ],
   "source": [
    "trainset = False\n",
    "dataset = tonic.datasets.NMNIST(save_to='../../Data/', train=trainset, transform=transform)\n",
    "loader = get_loader(dataset, kfold = None, shuffle=True)\n",
    "network.coding(loader, dataset.ordering, dataset.classes, filtering_threshold, training=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bc46b-7ee8-47f7-8be3-5e1fd8fac4ec",
   "metadata": {},
   "source": [
    "## Histogram classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aeac879-92df-43a0-8b23-e7d1b3dea9fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sensor_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m train_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Records/output/train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_sample_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Records/output/test/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_sample_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m testset \u001b[38;5;241m=\u001b[39m HOTS_Dataset(test_path, \u001b[43msensor_size\u001b[49m, transform\u001b[38;5;241m=\u001b[39mtonic\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNumpyAsType(\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(events[:,trainset\u001b[38;5;241m.\u001b[39mordering\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m     11\u001b[0m trainset \u001b[38;5;241m=\u001b[39m HOTS_Dataset(train_path, sensor_size, transform\u001b[38;5;241m=\u001b[39mtonic\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNumpyAsType(\u001b[38;5;28mint\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sensor_size' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import make_histogram_classification, HOTS_Dataset\n",
    "num_sample_test = 10000\n",
    "num_sample_train = 60000\n",
    "jitter = (None, None)\n",
    "\n",
    "train_path = f'../Records/output/train/{network.name}_{num_sample_train}_{jitter}/'\n",
    "test_path = f'../Records/output/test/{network.name}_{num_sample_test}_{jitter}/'\n",
    "\n",
    "testset = HOTS_Dataset(test_path, dataset.sensor_size, transform=tonic.transforms.NumpyAsType(int))\n",
    "print(events[:,trainset.ordering.index('p')])\n",
    "trainset = HOTS_Dataset(train_path, dataset.sensor_size, transform=tonic.transforms.NumpyAsType(int))\n",
    "\n",
    "\n",
    "\n",
    "score = make_histogram_classification(trainset, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd54d0-750d-46b0-9406-841921037639",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "- add plotlayer from neuronhots\n",
    "- add plot of the loss\n",
    "- add the MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de8a7e-4613-48a8-81a4-6065c1c84b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
