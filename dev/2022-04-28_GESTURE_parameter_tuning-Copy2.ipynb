{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1323e1fd-cd1d-4b6f-8de6-0a71f2d7a0b8",
   "metadata": {},
   "source": [
    "# [hotsline](https://github.com/AntoineGrimaldi/hotsline) algorithm to replicate results from [this paper](https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1)\n",
    "## Load events of the NMNIST dataset with [Tonic](https://tonic.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a757b26-fb4f-4947-a7ff-1751a686aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e167bc-5794-4989-8b83-eb671436db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/INT/grimaldi.a/Documents/projets/HOTS/hotsline/hots\n",
      "Tonic version installed -> 1.2.2\n",
      "Number of GPU devices available: 1\n",
      "GPU 1 named Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "%cd ../hots\n",
    "import tonic, torch, os, pickle\n",
    "from tqdm import tqdm\n",
    "from network import network\n",
    "from layer import mlrlayer\n",
    "from timesurface import timesurface\n",
    "from utils import apply_jitter, get_loader, get_sliced_loader, make_histogram_classification, HOTS_Dataset, fit_mlr, predict_mlr, score_classif_events, plotjitter, printfig, online_accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f'Tonic version installed -> {tonic.__version__}')\n",
    "\n",
    "print(f'Number of GPU devices available: {torch.cuda.device_count()}')\n",
    "for N_gpu in range(torch.cuda.device_count()):\n",
    "    print(f'GPU {N_gpu+1} named {torch.cuda.get_device_name(N_gpu)}')\n",
    "    \n",
    "record_path = '/envau/work/neopto/USERS/GRIMALDI/HOTS/hotsline/Records/'\n",
    "#record_path = '../Records/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91caa80b-9124-4c79-80f9-e6fad8ef1964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data/DVSGesture/metadata/gesture_1000_True_True\n",
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_1000_True_True/slice_metadata.h5.\n",
      "../../Data/DVSGesture/metadata/gesture_1000_True_True\n",
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_1000_True_True/slice_metadata.h5.\n",
      "../../Data/DVSGesture/metadata/gesture_1000_True_False\n",
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_1000_True_False/slice_metadata.h5.\n",
      "number of samples in the training set: 209\n",
      "number of samples in the testing set: 44\n"
     ]
    }
   ],
   "source": [
    "kfold_test = 5\n",
    "kfold_clust = 10\n",
    "ts_batch_size = 1000\n",
    "\n",
    "dataset_name = 'gesture'\n",
    "slicing_time_window = 1e6\n",
    "\n",
    "type_transform = tonic.transforms.NumpyAsType(int)\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True, transform=type_transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../../Data/', train=False, transform=type_transform)\n",
    "loader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_clust)\n",
    "trainloader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_test)\n",
    "num_sample_train = len(trainloader)\n",
    "testloader = get_sliced_loader(testset, slicing_time_window, dataset_name, False, only_first=True, kfold=kfold_test)\n",
    "num_sample_test = len(testloader)\n",
    "n_classes = len(testset.classes)\n",
    "print(f'number of samples in the training set: {len(trainloader)}')\n",
    "print(f'number of samples in the testing set: {len(testloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce802c4-e57b-4b22-98cf-0c5fd47633ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For tau equal: 60000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                  | 0/99 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m path \u001b[38;5;241m=\u001b[39m record_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetworks/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mhots\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mhots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltering_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiltering_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m hots\u001b[38;5;241m.\u001b[39mplotlayers()\n\u001b[1;32m     33\u001b[0m hots\u001b[38;5;241m.\u001b[39mcoding(trainloader, trainset\u001b[38;5;241m.\u001b[39mordering, trainset\u001b[38;5;241m.\u001b[39mclasses, filtering_threshold \u001b[38;5;241m=\u001b[39m filtering_threshold, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/projets/HOTS/hotsline/hots/network.py:106\u001b[0m, in \u001b[0;36mnetwork.clustering\u001b[0;34m(self, loader, ordering, filtering_threshold, ts_batch_size, device, record)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)):\n\u001b[0;32m--> 106\u001b[0m         all_ts, ind_filtered_timesurface \u001b[38;5;241m=\u001b[39m \u001b[43mtimesurface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensor_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensor_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_pola\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface_dimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltering_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiltering_threshold\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         n_star, _  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[L](all_ts, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m record:\n",
      "File \u001b[0;32m~/Documents/projets/HOTS/hotsline/hots/timesurface.py:64\u001b[0m, in \u001b[0;36mtimesurface\u001b[0;34m(events, sensor_size, ordering, surface_dimensions, tau, decay, filtering_threshold, drop_proba, ts_batch_size, load_number, previous_timestamp, device, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m     timesurface[timesurface \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m decay \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m     timesurface \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[43mtimestamp_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m)\n\u001b[1;32m     65\u001b[0m     timesurface[timesurface\u001b[38;5;241m<\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m))] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     66\u001b[0m all_surfaces[index, :, :, :] \u001b[38;5;241m=\u001b[39m timesurface\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name = 'homeohots'\n",
    "homeo = True\n",
    "#timestr = '2022-10-12'\n",
    "timestr = '2023-01-04'\n",
    "dataset_name = 'gesture'\n",
    "\n",
    "R_first = [4]\n",
    "N_layers = [2]\n",
    "n_first = [16]\n",
    "tau_first = [6e4, 3e4, 6e3]\n",
    "\n",
    "scores, parameters = [], []\n",
    "\n",
    "for lay in N_layers:\n",
    "    for R in R_first:\n",
    "        for tau in tau_first:\n",
    "            for N_neuron in n_first:\n",
    "                Rz = [R*2**Nl for Nl in range(lay)]\n",
    "                N_neuronz = [N_neuron*2**Nl for Nl in range(lay)]\n",
    "                N_pola = N_neuronz.copy()\n",
    "                N_pola.insert(0,2)\n",
    "                tauz = [tau*N_pola[Nl] for Nl in range(lay)]\n",
    "                print(f'For tau equal: {tau}')\n",
    "                \n",
    "                hots = network(name, dataset_name, timestr, trainset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo, record_path=record_path)\n",
    "                initial_name = hots.name\n",
    "                filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "                path = record_path+'networks/'+hots.name+'.pkl'\n",
    "                if not os.path.exists(path):\n",
    "                    hots.clustering(loader, trainset.ordering, filtering_threshold = filtering_threshold)\n",
    "                hots.plotlayers()\n",
    "                \n",
    "                hots.coding(trainloader, trainset.ordering, trainset.classes, filtering_threshold = filtering_threshold, training=True, verbose=False)\n",
    "                hots.coding(testloader, testset.ordering, testset.classes, filtering_threshold = filtering_threshold, training=False, verbose=False)\n",
    "                \n",
    "                jitter = (None, None)\n",
    "\n",
    "                train_path = f'{record_path}output/train/{hots.name}_{num_sample_train}_{jitter}/'\n",
    "                test_path = f'{record_path}output/test/{hots.name}_{num_sample_test}_{jitter}/'\n",
    "\n",
    "                testset_output = HOTS_Dataset(test_path, testset.sensor_size, testset.classes, transform=type_transform, dtype=trainset.dtype)\n",
    "                trainset_output = HOTS_Dataset(train_path, trainset.sensor_size, trainset.classes, transform=type_transform, dtype=trainset.dtype)\n",
    "                \n",
    "                score = make_histogram_classification(trainset_output, testset_output, N_neuronz[-1]) \n",
    "                \n",
    "                scores.append(score)\n",
    "                print(scores)\n",
    "                \n",
    "                parameters.append([lay, R, tau, N_neuron])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051407c0-c24d-4a7f-82a9-0dc34eff5dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e37181-fa1c-45ed-8349-2e8063e834d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58307d8-8460-47c4-9bd9-c2bc227b1a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e983f07-6877-44d4-8f83-c73b454b81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hots.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7c35c-e800-435c-afa4-b52c5f68366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 8#2 ** 5 + 1\n",
    "tau_cla_list_5 = [5e4, 7e4, 1e5, 2e5, 3e5, 5e5, 1e6, 2e6, 3e6, 4e6, 5e6, 7e6, 1e7, 2e7]\n",
    "\n",
    "for tau_cla in tau_cla_list_5:\n",
    "    results_path = f'../Records/LR_results/{hots.name}_{tau_cla}_{lr}_{betas}_{num_epochs}_{jitter}.pkl'\n",
    "    with open(results_path, 'rb') as file:\n",
    "        likelihood, true_target, timestamps = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ffb72-8ff3-4daf-a619-00e8c9208674",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .005\n",
    "tau_cla_list_5 = [5e4, 7e4, 1e5, 2e5, 3e5, 5e5, 1e6, 2e6, 3e6, 4e6, 5e6, 7e6, 1e7, 2e7]\n",
    "lr = .0005\n",
    "tau_cla_list_05 = [5e4, 7e4, ]\n",
    "lr = .0001\n",
    "tau_cla_list_01 = [3e7, 5e7, 7e7, 1e8, 2e8, 3e8, 4e8, 5e8, 7e8, 15e7, 25e7, 1e9]\n",
    "\n",
    "kfold_test = 10\n",
    "kfold_clust = 10\n",
    "ts_batch_size = 2000\n",
    "\n",
    "dataset_name = 'gesture'\n",
    "slicing_time_window = 1e6\n",
    "\n",
    "type_transform = tonic.transforms.NumpyAsType(int)\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True, transform=type_transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../../Data/', train=False, transform=type_transform)\n",
    "loader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_clust)\n",
    "trainloader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_test)\n",
    "num_sample_train = len(trainloader)\n",
    "testloader = get_sliced_loader(testset, slicing_time_window, dataset_name, False, only_first=True, kfold=kfold_test)\n",
    "num_sample_test = len(testloader)\n",
    "n_classes = len(testset.classes)\n",
    "print(f'number of samples in the training set: {len(trainloader)}')\n",
    "print(f'number of samples in the testing set: {len(testloader)}')\n",
    "\n",
    "name = 'homeohots'\n",
    "homeo = True\n",
    "timestr = '2022-04-22'\n",
    "\n",
    "Rz = [4, 8]\n",
    "N_neuronz = [16, 32]\n",
    "tauz = [5e4*2, 5e4*16]\n",
    "\n",
    "hots = network(name, dataset_name, timestr, trainset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo)\n",
    "\n",
    "#initial_name = hots.name\n",
    "\n",
    "filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "if not os.path.exists('../Records/'):\n",
    "    os.mkdir('../Records/')\n",
    "    os.mkdir('../Records/networks/')\n",
    "path = '../Records/networks/'+hots.name+'.pkl'\n",
    "print(path)\n",
    "if not os.path.exists(path):\n",
    "    hots.clustering(loader, trainset.ordering, filtering_threshold = filtering_threshold)\n",
    "    \n",
    "jitter = (None, None)\n",
    "num_workers = 0\n",
    "learning_rate = 0.0001\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 8#2 ** 5 + 1\n",
    "N_output_neurons = N_neuronz[-1]\n",
    "ts_size = (trainset.sensor_size[0],trainset.sensor_size[1],N_output_neurons)\n",
    "tau_cla_list = [5e4, 7e4]#, 1e5, 2e5, 3e5, 5e5, 1e6]\n",
    "\n",
    "train_path = f'../Records/output/train/{hots.name}_{num_sample_train}_{jitter}/'\n",
    "test_path = f'../Records/output/test/{hots.name}_{num_sample_test}_{jitter}/'\n",
    "\n",
    "hots.coding(trainloader, trainset.ordering, trainset.classes, training=True, verbose=False)\n",
    "hots.coding(testloader, testset.ordering, testset.classes, training=False, verbose=False)\n",
    "\n",
    "drop_proba = .5\n",
    "drop_transform = tonic.transforms.DropEvent(p = drop_proba)\n",
    "\n",
    "#trainset_output = HOTS_Dataset(train_path, trainset.sensor_size, trainset.classes, dtype=trainset.dtype, transform=tonic.transforms.Compose([drop_transform, type_transform]))\n",
    "#trainoutputloader = get_loader(trainset_output)\n",
    "#testset_output = HOTS_Dataset(test_path, testset.sensor_size, testset.classes, dtype=testset.dtype, transform=tonic.transforms.Compose([drop_transform, type_transform]))\n",
    "#testoutputloader = get_loader(testset_output)\n",
    "\n",
    "#score = make_histogram_classification(trainset_output, testset_output, N_neuronz[-1])\n",
    "score_nohomeo = 0\n",
    "score = 0\n",
    "#print(testset_output.classes)\n",
    "\n",
    "for tau_cla in tau_cla_list_01:\n",
    "\n",
    "    model_path = f'../Records/networks/{hots.name}_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{jitter}.pkl'\n",
    "    results_path = f'../Records/LR_results/{hots.name}_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{jitter}.pkl'\n",
    "    #print(f'Number of samples in the trainset set: {len(trainoutputloader)}')\n",
    "    \n",
    "    #classif_layer, losses = fit_mlr(trainoutputloader, model_path, tau_cla, learning_rate, betas, num_epochs, ts_size, trainset.ordering, len(trainset.classes), ts_batch_size = ts_batch_size)\n",
    "    #plt.plot(losses)\n",
    "    #likelihood, true_target, timestamps = predict_mlr(classif_layer,tau_cla,testoutputloader,results_path,ts_size,testset_output.ordering,  ts_batch_size = ts_batch_size)\n",
    "    with open(results_path, 'rb') as file:\n",
    "        likelihood, true_target, timestamps = pickle.load(file)\n",
    "    #for i in range(len(likelihood)):\n",
    "    #    for kernel in range(likelihood[i].shape[1]):\n",
    "    #        plt.plot(likelihood[i][:,kernel])\n",
    "    meanac, onlinac, lastac = score_classif_events(likelihood, true_target, n_classes, original_accuracy = score, original_accuracy_nohomeo = score_nohomeo)#, figure_name = 'nmnist_online.pdf')\n",
    "    print(f'For tau = {tau_cla} last accuracy: {lastac*100}% - mean accuracy: {meanac*100}%')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db9f61d3-0cb8-4afa-ada4-40747ae495d8",
   "metadata": {},
   "source": [
    "hots.plotlayers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07d35d-8d14-4b64-b5cc-e90bd70e6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "hots.plotlayers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4950206-7429-4e86-8138-6f112f591156",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = classif_layer.linear.weight.data.cpu().numpy()\n",
    "fig, ax = plt.subplots(N_output_neurons, kernels.shape[0], figsize=(30, 90))\n",
    "for n in range(kernels.shape[0]):\n",
    "    kernel = kernels[n].reshape(trainset.sensor_size[0],trainset.sensor_size[1], N_output_neurons)\n",
    "    for p in range(N_output_neurons):\n",
    "        ax[p, n].imshow(kernel[:,:,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6ef67-582c-473d-be02-a93885437f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic, torch, os\n",
    "from hots.network import network\n",
    "from hots.utils import apply_jitter, get_loader, get_sliced_loader, make_histogram_classification, HOTS_Dataset, fit_mlr, predict_mlr, score_classif_events\n",
    "import numpy as np\n",
    "\n",
    "print(f'Tonic version installed -> {tonic.__version__}')\n",
    "\n",
    "print(f'Number of GPU devices available: {torch.cuda.device_count()}')\n",
    "for N_gpu in range(torch.cuda.device_count()):\n",
    "    print(f'GPU {N_gpu+1} named {torch.cuda.get_device_name(N_gpu)}')\n",
    "    \n",
    "kfold_test = None\n",
    "kfold_clust = 10\n",
    "ts_batch_size = 1000\n",
    "\n",
    "dataset_name = 'gesture'\n",
    "slicing_time_window = 1e6\n",
    "\n",
    "type_transform = tonic.transforms.NumpyAsType(int)\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True, transform=type_transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../../Data/', train=False, transform=type_transform)\n",
    "loader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_clust)\n",
    "trainloader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_test)\n",
    "num_sample_train = len(trainloader)\n",
    "testloader = get_sliced_loader(testset, slicing_time_window, dataset_name, False, only_first=True, kfold=kfold_test)\n",
    "num_sample_test = len(testloader)\n",
    "n_classes = len(testset.classes)\n",
    "print(f'number of samples in the training set: {len(trainloader)}')\n",
    "print(f'number of samples in the testing set: {len(testloader)}')\n",
    "\n",
    "name = 'homeohots'\n",
    "homeo = True\n",
    "timestr = '2022-04-22'\n",
    "\n",
    "Rz = [4, 8]\n",
    "N_neuronz = [16, 32]\n",
    "tauz = [5e4*2, 5e4*16]\n",
    "\n",
    "hots = network(name, dataset_name, timestr, trainset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo)\n",
    "\n",
    "#initial_name = hots.name\n",
    "\n",
    "filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "if not os.path.exists('../Records/'):\n",
    "    os.mkdir('../Records/')\n",
    "    os.mkdir('../Records/networks/')\n",
    "path = '../Records/networks/'+hots.name+'.pkl'\n",
    "if not os.path.exists(path):\n",
    "    hots.clustering(loader, trainset.ordering, filtering_threshold = filtering_threshold)\n",
    "    \n",
    "jitter = (None, None)\n",
    "num_workers = 0\n",
    "learning_rate = 0.0001\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 11#2 ** 5 + 1\n",
    "N_output_neurons = N_neuronz[-1]\n",
    "ts_size = (trainset.sensor_size[0],trainset.sensor_size[1],N_output_neurons)\n",
    "tau_cla_list = [1e5, 5e5, 1e6, 5e6, 1e7, 5e7, 1e8]\n",
    "\n",
    "train_path = f'../Records/output/train/{hots.name}_{num_sample_train}_{jitter}/'\n",
    "test_path = f'../Records/output/test/{hots.name}_{num_sample_test}_{jitter}/'\n",
    "\n",
    "print(hots.name)\n",
    "\n",
    "hots.coding(trainloader, trainset.ordering, trainset.classes, training=True, verbose=False)\n",
    "hots.coding(testloader, testset.ordering, testset.classes, training=False, verbose=False)\n",
    "\n",
    "drop_proba = .9\n",
    "if drop_proba:\n",
    "    drop_transform = tonic.transforms.DropEvent(p = drop_proba)\n",
    "    full_drop_transform = tonic.transforms.Compose([drop_transform, type_transform])\n",
    "else: full_drop_transform = type_transform\n",
    "\n",
    "kfold_mlr = 10\n",
    "\n",
    "trainset_output = HOTS_Dataset(train_path, trainset.sensor_size, trainset.classes, dtype=trainset.dtype, transform=full_drop_transform)\n",
    "trainoutputloader = get_loader(trainset_output, kfold = kfold_mlr)\n",
    "testset_output = HOTS_Dataset(test_path, testset.sensor_size, testset.classes, dtype=testset.dtype, transform=type_transform)\n",
    "testoutputloader = get_loader(testset_output, kfold = kfold_mlr)\n",
    "\n",
    "score = make_histogram_classification(trainset_output, testset_output, N_neuronz[-1])\n",
    "score_nohomeo = make_histogram_classification(trainset_output, testset_output, N_neuronz[-1])\n",
    "\n",
    "print(score, score_nohomeo)\n",
    "\n",
    "for tau_cla in tau_cla_list:\n",
    "\n",
    "    model_path = f'../Records/networks/{hots.name}_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{jitter}_{drop_proba}.pkl'\n",
    "    results_path = f'../Records/LR_results/{hots.name}_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{jitter}_{drop_proba}.pkl'\n",
    "    print(f'Number of samples in the trainset set: {len(trainoutputloader)}')\n",
    "    \n",
    "    print(len(trainoutputloader))\n",
    "    \n",
    "    classif_layer, losses = fit_mlr(trainoutputloader, model_path, tau_cla, learning_rate, betas, num_epochs, ts_size, trainset.ordering, len(trainset.classes), ts_batch_size = ts_batch_size)\n",
    "    likelihood, true_target, timestamps = predict_mlr(classif_layer,tau_cla,testoutputloader,results_path,ts_size,testset_output.ordering,  ts_batch_size = ts_batch_size)\n",
    "    meanac, onlinac, lastac = score_classif_events(likelihood, true_target, n_classes, original_accuracy = score, original_accuracy_nohomeo = score_nohomeo)#, figure_name = 'nmnist_online.pdf')\n",
    "    print(f'For tau = {tau_cla} last accuracy: {lastac*100}% - mean accuracy: {meanac*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64651590-32cc-448b-8a5f-08712cb70972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
