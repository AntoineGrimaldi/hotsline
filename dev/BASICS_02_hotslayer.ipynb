{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146e9eb1-7f15-4122-a866-cd3061b73da0",
   "metadata": {},
   "source": [
    "# BASICS 02 - One layer of HOTS in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a75ff8-780a-400b-9925-515f4a2274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6346880-ad3c-4a2c-a39a-1a8842c3ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antoine/homhots/hotsline/hots\n",
      " Tonic version installed -> 1.0.15\n"
     ]
    }
   ],
   "source": [
    "%cd ../hots\n",
    "import tonic, torch\n",
    "from timesurface import timesurface\n",
    "from utils import get_loader, get_dataset_info\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from layer import hotslayer\n",
    "\n",
    "print(f' Tonic version installed -> {tonic.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcbdeff-9a63-4aed-a950-073d75af4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tonic.transforms.NumpyAsType(int)\n",
    "dataset = tonic.datasets.NMNIST(save_to='../../Data/', train=True, transform=transform)\n",
    "loader = get_loader(dataset, shuffle=True)\n",
    "#get_dataset_info(dataset, properties = ['time', 'mean_isi', 'nb_events']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90e0933-87c0-4d98-855c-ed4ad5098c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 4\n",
    "ts_size = (2*R+1)\n",
    "tau = 5e3#7e2\n",
    "\n",
    "transform = tonic.transforms.Compose([tonic.transforms.ToTimesurface(sensor_size=dataset.sensor_size, tau=tau, decay='exp', surface_dimensions=(ts_size,ts_size))])\n",
    "dataset = tonic.datasets.NMNIST(save_to='../../Data/', train=True, transform=transform)\n",
    "loader = get_loader(dataset, kfold=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43405e6-0758-4ece-b1c1-e7a6f692699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device -> cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▍                                                                                                                                                                                                                                     | 3/200 [00:22<25:08,  7.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     layer \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m all_ts, target \u001b[38;5;129;01min\u001b[39;00m tqdm(loader):\n\u001b[0;32m---> 19\u001b[0m         n_star \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclustering_flag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         output_pola\u001b[38;5;241m.\u001b[39mappend(n_star)\n\u001b[1;32m     21\u001b[0m layer \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/homhots/hotsline/hots/layer.py:34\u001b[0m, in \u001b[0;36mhotslayer.forward\u001b[0;34m(self, all_ts, clustering_flag)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynapses\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[n_star_ev,:] \u001b[38;5;241m=\u001b[39m Ck \u001b[38;5;241m+\u001b[39m alpha\u001b[38;5;241m*\u001b[39mbeta[n_star_ev]\u001b[38;5;241m*\u001b[39m(ts \u001b[38;5;241m-\u001b[39m Ck)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# learning rule from Lagorce 2017\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m#self.synapses[:,n_star] = Ck + alpha*(TS - simil[closest_proto_idx]*Ck)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumhisto[n_star_ev] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m         n_star[iev] \u001b[38;5;241m=\u001b[39m n_star_ev\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R = 4\n",
    "n_pola = 2\n",
    "ts_size = (2*R+1)**2*n_pola\n",
    "n_neurons = 32\n",
    "tau = 5e3#7e2\n",
    "verbose = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if verbose: print(f'device -> {device}')\n",
    "\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_pola = []\n",
    "    layer = hotslayer(ts_size, n_neurons, device=device)\n",
    "    #layer.learning_flag = False\n",
    "    layer = layer.to(device)\n",
    "    for all_ts, target in tqdm(loader):\n",
    "        n_star = layer(all_ts.to(device).squeeze(0), clustering_flag = True)\n",
    "        output_pola.append(n_star)\n",
    "layer = layer.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cadc7-ca7f-4e64-95f4-aabb02eaf89d",
   "metadata": {},
   "source": [
    "# TODO: check for loss diminution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54cb20-2630-47fc-a4ff-e609a8a18855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
