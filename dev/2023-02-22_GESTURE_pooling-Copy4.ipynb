{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1323e1fd-cd1d-4b6f-8de6-0a71f2d7a0b8",
   "metadata": {},
   "source": [
    "# [hotsline](https://github.com/AntoineGrimaldi/hotsline) algorithm to replicate results from [this paper](https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a757b26-fb4f-4947-a7ff-1751a686aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e167bc-5794-4989-8b83-eb671436db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/INT/grimaldi.a/Documents/projets/HOTS/hotsline/hots\n",
      "Tonic version installed -> 1.2.2\n",
      "Number of GPU devices available: 1\n",
      "GPU 1 named Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "%cd ../hots\n",
    "import tonic, torch, os, pickle, glob\n",
    "from tqdm import tqdm\n",
    "from network import network\n",
    "from layer import mlrlayer\n",
    "from timesurface import timesurface\n",
    "from utils import apply_jitter, get_loader, get_sliced_loader, make_histogram_classification, HOTS_Dataset, fit_mlr, predict_mlr, score_classif_events, plotjitter, printfig, online_accuracy, make_and_display_ts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f'Tonic version installed -> {tonic.__version__}')\n",
    "\n",
    "print(f'Number of GPU devices available: {torch.cuda.device_count()}')\n",
    "for N_gpu in range(torch.cuda.device_count()):\n",
    "    print(f'GPU {N_gpu+1} named {torch.cuda.get_device_name(N_gpu)}')\n",
    "    \n",
    "#record_path = '/envau/work/neopto/USERS/GRIMALDI/HOTS/hotsline/Records/'\n",
    "record_path = '../Records/'\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91caa80b-9124-4c79-80f9-e6fad8ef1964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_2000_True_True/slice_metadata.h5.\n",
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_2000_True_False/slice_metadata.h5.\n",
      "--- Slicing dataset done ---\n"
     ]
    }
   ],
   "source": [
    "kfold_test = None\n",
    "kfold_clust = 10\n",
    "ts_batch_size = int(1e5)\n",
    "\n",
    "dataset_name = 'gesture'\n",
    "slicing_time_window = 2e6\n",
    "only_first = True\n",
    "\n",
    "#to uncomment \n",
    "trainset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../../Data/', train=False)\n",
    "trainloader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=only_first)\n",
    "testloader = get_sliced_loader(testset, slicing_time_window, dataset_name, False, only_first=only_first)\n",
    "print('--- Slicing dataset done ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1595e803-7ab2-4ff8-9dc3-6614fda54ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_2000_True_True/slice_metadata.h5.\n",
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_2000_True_True/slice_metadata.h5.\n",
      "Metadata read from ../../Data/DVSGesture/metadata/gesture_2000_True_False/slice_metadata.h5.\n",
      "number of samples in the training set: 1077\n",
      "number of samples in the testing set: 264\n"
     ]
    }
   ],
   "source": [
    "type_transform = tonic.transforms.NumpyAsType(int)\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True, transform=type_transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../../Data/', train=False, transform=type_transform)\n",
    "loader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=only_first, kfold=kfold_clust)\n",
    "trainloader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=only_first, kfold=kfold_test)\n",
    "num_sample_train = len(trainloader)\n",
    "testloader = get_sliced_loader(testset, slicing_time_window, dataset_name, False, only_first=only_first, kfold=kfold_test)\n",
    "num_sample_test = len(testloader)\n",
    "n_classes = len(testset.classes)\n",
    "print(f'number of samples in the training set: {len(trainloader)}')\n",
    "print(f'number of samples in the testing set: {len(testloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1174d5b0-aa6e-474e-b43f-1fc2012ffc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hots.layer import hotslayer\n",
    "from tqdm import tqdm\n",
    "from hots.timesurface import timesurface\n",
    "from hots.utils import entropy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, os\n",
    "import pickle\n",
    "\n",
    "def entropy(timesurface):\n",
    "    hist = torch.histc(timesurface, bins = 256, min = 0, max = 1)\n",
    "    return -torch.nansum(hist*torch.log2(hist))\n",
    "\n",
    "class network_pooling(object):\n",
    "\n",
    "    def __init__(self,  name,\n",
    "                        dataset_name,\n",
    "                        timestr, # date of creation of the network \n",
    "                        sensor_size, # \n",
    "                        nb_neurons = (4,8,16), # architecture of the network (default=Lagorce2017)\n",
    "                        # parameters of time-surfaces and datasets\n",
    "                        tau = (1e1,1e2,1e3), #time constant for exponential decay in millisec\n",
    "                        R = (2,4,8), # parameter defining the spatial size of the time surface\n",
    "                        homeo = True, # parameters for homeostasis (None is no homeo rule)\n",
    "                        to_record = False,\n",
    "                        record_path = '../Records/',\n",
    "                        device = 'cuda',\n",
    "                ):\n",
    "        assert len(nb_neurons) == len(R) & len(nb_neurons) == len(tau)\n",
    "        \n",
    "        self.name = f'{timestr}_{dataset_name}_{name}_{homeo}_{nb_neurons}_{tau}_{R}'\n",
    "        nb_layers = len(nb_neurons)\n",
    "        self.n_pola = [nb_neurons[L] for L in range(nb_layers-1)]\n",
    "        self.n_pola.insert(0,2)\n",
    "        # pooling\n",
    "        self.channel_size = [(sensor_size[0]//(2*L), sensor_size[1]//(2*L)) for L in range(1,nb_layers)]\n",
    "        self.channel_size.insert(0,(sensor_size[0], sensor_size[1]))\n",
    "        self.tau = tau\n",
    "        self.R = R\n",
    "        self.record_path = record_path\n",
    "        \n",
    "        # pooling\n",
    "        for L in range(nb_layers):\n",
    "            assert (2*(L+1))**2 <= self.channel_size[L][0]*self.channel_size[L][1]\n",
    "        \n",
    "        path = self.record_path+'networks/'+self.name+'.pkl'\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as file:\n",
    "                my_network = pickle.load(file)\n",
    "            self.layers = my_network.layers\n",
    "            for L in range(len(self.layers)):\n",
    "                self.layers[L] = self.layers[L].to(device)\n",
    "            \n",
    "        else:\n",
    "            self.layers = [hotslayer((2*R[L]+1)**2*self.n_pola[L], nb_neurons[L], homeostasis=homeo, device=device) for L in range(nb_layers)]\n",
    "            \n",
    "    def clustering(self, loader, ordering, filtering_threshold = None, ts_batch_size = None, device = 'cuda', record = False):\n",
    "        path = self.record_path+'networks/'+self.name+'.pkl'\n",
    "        if not os.path.exists(path):\n",
    "            p_index = ordering.index('p')\n",
    "            x_index = ordering.index('x')\n",
    "            y_index = ordering.index('y')\n",
    "            \n",
    "            for L in range(len(self.tau)):\n",
    "                self.layers[L] = self.layers[L].to(device)\n",
    "                self.layers[L].cumhisto = self.layers[L].cumhisto.to(device)\n",
    "            \n",
    "            if record:\n",
    "                ent = []\n",
    "                loss = []\n",
    "                delta_w = []\n",
    "                homeostasis = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                if not filtering_threshold: filtering_threshold = [None for L in range(len(self.tau))]\n",
    "                \n",
    "                for events, target in tqdm(loader):\n",
    "                    events = events.squeeze(0)\n",
    "                    if record:\n",
    "                        previous_dic = [self.layers[L].synapses.weight.data.T.detach().clone() for L in range(len(self.tau))]\n",
    "                    if ts_batch_size and len(events)>ts_batch_size:\n",
    "                        nb_batch = len(events)//ts_batch_size+1\n",
    "                        for L in range(len(self.tau)):\n",
    "                            previous_timestamp = []\n",
    "                            outputs = torch.Tensor([])\n",
    "                            ind_outputs = torch.Tensor([])\n",
    "                            for load_nb in range(nb_batch):\n",
    "                                all_ts, ind_filtered_timesurface, previous_timestamp = timesurface(events, (self.channel_size[L][0], self.channel_size[L][1], self.n_pola[L]), ordering, tau = self.tau[L], surface_dimensions=[2*self.R[L]+1,2*self.R[L]+1], filtering_threshold = filtering_threshold[L], ts_batch_size = ts_batch_size, load_number = load_nb, previous_timestamp = previous_timestamp, device = device)\n",
    "                                n_star, _, beta = self.layers[L](all_ts, True)\n",
    "                                outputs = torch.hstack([outputs,n_star]) if outputs.shape[0]>0 else n_star\n",
    "                                ind_outputs = torch.hstack([ind_outputs,ind_filtered_timesurface+load_nb*ts_batch_size]) if ind_outputs.shape[0]>0 else ind_filtered_timesurface\n",
    "                                if record:\n",
    "                                    proto_ts = all_ts.detach().clone()\n",
    "                                    kernels = self.layers[L].synapses.weight.data.T\n",
    "                                    DIFF = 0\n",
    "                                    for ev in range(len(n_star)):\n",
    "                                        proto_ts[ev,:,:,:] = torch.reshape(kernels[:,int(n_star[ev].cpu())], (self.n_pola[L], 2*self.R[L]+1, 2*self.R[L]+1))\n",
    "                                        diff = torch.linalg.norm(all_ts[ev,:,:,:]-proto_ts[ev,:,:,:])\n",
    "                                        DIFF += diff.mean()\n",
    "                                    DIFF/=len(n_star)\n",
    "                                    loss.append(DIFF.cpu())\n",
    "                                    ent.append(entropy(kernels))\n",
    "                                    delta_w.append((kernels-previous_dic[L]).abs().mean().cpu())\n",
    "                                    homeostasis.append((self.layers[L].cumhisto/self.layers[L].cumhisto.sum()-1/kernels.shape[1]).abs().mean().cpu())\n",
    "                                del all_ts\n",
    "                                torch.cuda.empty_cache()\n",
    "                            events = events[ind_outputs,:]\n",
    "                            events[:,p_index] = outputs.cpu()\n",
    "                            events[:,x_index] = torch.div(events[:,x_index], 2, rounding_mode='floor')\n",
    "                            events[:,y_index] = torch.div(events[:,y_index], 2, rounding_mode='floor')\n",
    "                            if events.shape[0]==0: break\n",
    "                    else:    \n",
    "                        for L in range(len(self.tau)):\n",
    "                            all_ts, ind_filtered_timesurface = timesurface(events, (self.channel_size[L][0], self.channel_size[L][1], self.n_pola[L]), ordering, tau = self.tau[L], surface_dimensions=[2*self.R[L]+1,2*self.R[L]+1], filtering_threshold = filtering_threshold[L], device=device)\n",
    "                            n_star, _, beta  = self.layers[L](all_ts, True)\n",
    "                            if record:\n",
    "                                proto_ts = all_ts.detach().clone()\n",
    "                                kernels = self.layers[L].synapses.weight.data.T\n",
    "                                DIFF = 0\n",
    "                                for ev in range(len(n_star)):\n",
    "                                    proto_ts[ev,:,:,:] = torch.reshape(kernels[:,int(n_star[ev].cpu())], (self.n_pola[L], 2*self.R[L]+1, 2*self.R[L]+1))\n",
    "                                    diff = torch.linalg.norm(all_ts[ev,:,:,:]-proto_ts[ev,:,:,:])\n",
    "                                    DIFF += diff.mean()\n",
    "                                DIFF/=len(n_star)\n",
    "                                loss.append(DIFF.cpu())\n",
    "                                ent.append(entropy(kernels))\n",
    "                                delta_w.append((kernels-previous_dic[L]).abs().mean().cpu())\n",
    "                                homeostasis.append((self.layers[L].cumhisto/self.layers[L].cumhisto.sum()-1/kernels.shape[1]).abs().mean().cpu())\n",
    "                            del all_ts\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                            events = events[ind_filtered_timesurface,:]\n",
    "                            events[:,p_index] = n_star.cpu()\n",
    "                            events[:,x_index] = torch.div(events[:,x_index], 2, rounding_mode='floor')\n",
    "                            events[:,y_index] = torch.div(events[:,y_index], 2, rounding_mode='floor')\n",
    "                            if events.shape[0]==0: break\n",
    "\n",
    "            with open(path, 'wb') as file:\n",
    "                pickle.dump(self, file, pickle.HIGHEST_PROTOCOL)\n",
    "            if record:\n",
    "                path = self.record_path+'networks/'+self.name+'_recorded_parameters.pkl'\n",
    "                with open(path, 'wb') as file:\n",
    "                    pickle.dump([loss, ent, delta_w, homeostasis], file, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "    def coding(self, loader, ordering, classes, training, ts_batch_size = None, filtering_threshold = None, jitter=(None,None), layer_threshold = None, device = 'cuda', verbose=True):\n",
    "        \n",
    "        #homeostatic gain control is used only for the clustering phase\n",
    "        for L in range(len(self.tau)):\n",
    "            self.layers[L].homeo_flag = False\n",
    "            self.layers[L] = self.layers[L].to(device)\n",
    "            self.layers[L].cumhisto = self.layers[L].cumhisto.to(device)\n",
    "        \n",
    "        if not filtering_threshold: filtering_threshold = [None for L in range(len(self.tau))]\n",
    "        if not layer_threshold: layer_threshold = [None for L in range(len(self.tau))]\n",
    "        \n",
    "        p_index = ordering.index('p')\n",
    "        x_index = ordering.index('x')\n",
    "        y_index = ordering.index('y')\n",
    "        \n",
    "        if training:\n",
    "            output_path = self.record_path+f'output/train/{self.name}_{len(loader)}_{jitter}/'\n",
    "        else: output_path = self.record_path+f'output/test/{self.name}_{len(loader)}_{jitter}/'\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            if verbose:\n",
    "                print(f'this dataset have already been processed, check at: \\n {output_path}')\n",
    "        else:\n",
    "            for classe in classes:\n",
    "                os.makedirs(output_path+f'{classe}')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                nb = 0\n",
    "                for events, target in tqdm(loader):\n",
    "                    events = events.squeeze(0)\n",
    "                    if ts_batch_size and len(events)>ts_batch_size:\n",
    "                        nb_batch = len(events)//ts_batch_size+1\n",
    "                        for L in range(len(self.tau)):\n",
    "                            previous_timestamp = []\n",
    "                            outputs = torch.Tensor([])\n",
    "                            ind_outputs = torch.Tensor([])\n",
    "                            for load_nb in range(nb_batch):\n",
    "                                all_ts, ind_filtered_timesurface, previous_timestamp = timesurface(events, (self.channel_size[L][0], self.channel_size[L][1], self.n_pola[L]), ordering, tau = self.tau[L], surface_dimensions=[2*self.R[L]+1,2*self.R[L]+1], filtering_threshold = filtering_threshold[L], ts_batch_size = ts_batch_size, load_number = load_nb, previous_timestamp = previous_timestamp, device = device)\n",
    "                                n_star, ind_filtered_layer, beta = self.layers[L](all_ts, False)\n",
    "                                ind_to_keep = ind_filtered_timesurface[ind_filtered_layer]\n",
    "                                outputs = torch.hstack([outputs,n_star]) if outputs.shape[0]>0 else n_star\n",
    "                                ind_outputs = torch.hstack([ind_outputs,ind_to_keep+load_nb*ts_batch_size]) if ind_outputs.shape[0]>0 else ind_to_keep\n",
    "                                del all_ts\n",
    "                                torch.cuda.empty_cache()\n",
    "                            events = events[ind_outputs,:]\n",
    "                            events[:,p_index] = outputs.cpu()\n",
    "                            events[:,x_index] = torch.div(events[:,x_index], 2, rounding_mode='floor')\n",
    "                            events[:,y_index] = torch.div(events[:,y_index], 2, rounding_mode='floor')\n",
    "                            if events.shape[0]==0: \n",
    "                                complete_flag = False\n",
    "                                break\n",
    "                            else:\n",
    "                                complete_flag = True\n",
    "                    else:\n",
    "                        for L in range(len(self.tau)):\n",
    "                            all_ts, ind_filtered_timesurface = timesurface(events, (self.channel_size[L][0], self.channel_size[L][1], self.n_pola[L]), ordering, tau = self.tau[L], surface_dimensions=[2*self.R[L]+1,2*self.R[L]+1], filtering_threshold = filtering_threshold[L], device=device)\n",
    "                            n_star, ind_filtered_layer, beta = self.layers[L](all_ts, False)\n",
    "                            events = events[ind_filtered_timesurface,:]\n",
    "                            events[:,p_index] = n_star.cpu()\n",
    "                            events = events[ind_filtered_layer,:]\n",
    "                            events[:,x_index] = torch.div(events[:,x_index], 2, rounding_mode='floor')\n",
    "                            events[:,y_index] = torch.div(events[:,y_index], 2, rounding_mode='floor')\n",
    "                            del all_ts\n",
    "                            torch.cuda.empty_cache()\n",
    "                            if events.shape[0]==0: \n",
    "                                complete_flag = False\n",
    "                                break\n",
    "                            else:\n",
    "                                complete_flag = True\n",
    "                    if complete_flag:\n",
    "                        np.save(output_path+f'{classes[target]}/{nb}', events)\n",
    "                    nb+=1\n",
    "                    \n",
    "                    \n",
    "    def plotlayers(self, maxpol=None, hisiz=2, yhis=0.3):\n",
    "        '''\n",
    "        '''\n",
    "        N = []\n",
    "        P = [2]\n",
    "        R2 = []\n",
    "        kernels = []\n",
    "        for L in range(len(self.tau)):\n",
    "            kernels.append(self.layers[L].synapses.weight.data.T.cpu().numpy())\n",
    "            N.append(int(kernels[L].shape[1]))\n",
    "            if L>0:\n",
    "                P.append(int(kernels[L-1].shape[1]))\n",
    "            R2.append(int(kernels[L].shape[0]/P[L]))\n",
    "        if maxpol is None:\n",
    "            maxpol=P[-1]\n",
    "\n",
    "        fig = plt.figure(figsize=(16,9))\n",
    "        gs = fig.add_gridspec(np.sum(P)+hisiz, np.sum(N)+len(self.tau)-1, wspace=0.05, hspace=0.05)\n",
    "        if self.layers[-1].homeo_flag:\n",
    "            fig.suptitle('Unsupervised clustering with homeostasis', size=20, y=0.95)\n",
    "        else:\n",
    "            fig.suptitle('Unsupervised clustering for original HOTS', size=20, y=0.95)\n",
    "\n",
    "        for L in range(len(self.tau)):\n",
    "            ax = fig.add_subplot(gs[:hisiz, int(np.sum(N[:L]))+1*L:int(np.sum(N[:L+1]))+L*1])\n",
    "            plt.bar(np.arange(N[L]), (self.layers[L].cumhisto/torch.sum(self.layers[L].cumhisto)).cpu(), width=1, align='edge', ec=\"k\")\n",
    "            ax.set_xticks(())\n",
    "            ax.set_title('Layer '+str(L+1), fontsize=16)\n",
    "            plt.xlim([0,N[L]])\n",
    "            yhis = 1.1*max(self.layers[L].cumhisto/torch.sum(self.layers[L].cumhisto)).cpu()\n",
    "            plt.ylim([0,yhis])\n",
    "\n",
    "            for k in range(N[L]):\n",
    "                vmaxi = max(kernels[L][:,k])\n",
    "                for j in range(P[L]):\n",
    "                    if j>maxpol-1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        axi = fig.add_subplot(gs[j+hisiz,k+1*L+int(np.sum(N[:L]))])\n",
    "                        krnl = kernels[L][j*R2[L]:(j+1)*R2[L],k].reshape((int(np.sqrt(R2[L])), int(np.sqrt(R2[L]))))\n",
    "\n",
    "                        axi.imshow(krnl, vmin=0, vmax=vmaxi, cmap=plt.cm.plasma, interpolation='nearest')\n",
    "                        axi.set_xticks(())\n",
    "                        axi.set_yticks(())\n",
    "        plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def plotlearning(self, width_fig = 30):\n",
    "        path = self.record_path+'networks/'+self.name+'_recorded_parameters.pkl'\n",
    "        with open(path, 'rb') as file:\n",
    "            loss, entropy, delta_w, homeostasis = pickle.load(file)\n",
    "            \n",
    "        n_layers = len(self.tau)\n",
    "        fig, axs = plt.subplots(n_layers,4, figsize=(width_fig,n_layers*width_fig//4))\n",
    "        for L in range(n_layers):\n",
    "            loss_layer = loss[L::n_layers]\n",
    "            entropy_layer = entropy[L::n_layers]\n",
    "            delta_w_layer = delta_w[L::n_layers]\n",
    "            homeostasis_layer = homeostasis[L::n_layers]\n",
    "            axs[L,0].plot(loss_layer)\n",
    "            axs[L,1].plot(entropy_layer)\n",
    "            axs[L,2].plot(delta_w_layer)\n",
    "            axs[L,3].plot(homeostasis_layer)\n",
    "            if L == 0:\n",
    "                axs[L,0].set_title('average loss')\n",
    "                axs[L,1].set_title('average entropy values for the time surfaces')\n",
    "                axs[L,2].set_title('average gradient of the weights')\n",
    "                axs[L,3].set_title('average homeostasic gain')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce802c4-e57b-4b22-98cf-0c5fd47633ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                  | 0/99 [08:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'beta' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m path \u001b[38;5;241m=\u001b[39m record_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetworks/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mhots\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mhots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltering_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiltering_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mts_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m hots\u001b[38;5;241m.\u001b[39mplotlayers();\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mnetwork_pooling.clustering\u001b[0;34m(self, loader, ordering, filtering_threshold, ts_batch_size, device, record)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m load_nb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_batch):\n\u001b[1;32m     89\u001b[0m     all_ts, ind_filtered_timesurface, previous_timestamp \u001b[38;5;241m=\u001b[39m timesurface(events, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_size[L][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_size[L][\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pola[L]), ordering, tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau[L], surface_dimensions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[L]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[L]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], filtering_threshold \u001b[38;5;241m=\u001b[39m filtering_threshold[L], ts_batch_size \u001b[38;5;241m=\u001b[39m ts_batch_size, load_number \u001b[38;5;241m=\u001b[39m load_nb, previous_timestamp \u001b[38;5;241m=\u001b[39m previous_timestamp, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[0;32m---> 90\u001b[0m     n_star, _, beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack([outputs,n_star]) \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m n_star\n\u001b[1;32m     92\u001b[0m     ind_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack([ind_outputs,ind_filtered_timesurface\u001b[38;5;241m+\u001b[39mload_nb\u001b[38;5;241m*\u001b[39mts_batch_size]) \u001b[38;5;28;01mif\u001b[39;00m ind_outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m ind_filtered_timesurface\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/projets/HOTS/hotsline/hots/layer.py:47\u001b[0m, in \u001b[0;36mhotslayer.forward\u001b[0;34m(self, all_ts, clustering_flag, layer_threshold)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(all_ts\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_star, indices, \u001b[43mbeta\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'beta' referenced before assignment"
     ]
    }
   ],
   "source": [
    "name = 'homeohots_pool'\n",
    "homeo = True\n",
    "timestr = '2023-04-14'\n",
    "dataset_name = 'gesture'\n",
    "\n",
    "Rz = [4, 4]\n",
    "N_neuronz = [32, 64]\n",
    "#layer_threshold = [0.05, 0.1]\n",
    "tauz = [5e3*2, 5e3*N_neuronz[0]]#/(2*Rz[0]+1)**2]\n",
    "\n",
    "hots = network_pooling(name, dataset_name, timestr, trainset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo, record_path=record_path)\n",
    "\n",
    "initial_name = hots.name\n",
    "\n",
    "filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "if not os.path.exists(record_path):\n",
    "    os.mkdir(record_path)\n",
    "    os.mkdir(record_path+'networks/')\n",
    "path = record_path+'networks/'+hots.name+'.pkl'\n",
    "if not os.path.exists(path):\n",
    "    hots.clustering(loader, trainset.ordering, filtering_threshold = filtering_threshold, ts_batch_size = ts_batch_size, record = True, device = device)\n",
    "hots.plotlayers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed9d4d-08ae-4dd1-abf8-e379fb40b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hots.plotlearning();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc6a40-428e-4326-869d-64dafeb072f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = (None, None)\n",
    "N_output_neurons = N_neuronz[-1]\n",
    "\n",
    "hots.coding(trainloader, trainset.ordering, trainset.classes, layer_threshold = layer_threshold, training=True, ts_batch_size = ts_batch_size, verbose=False, device = device)\n",
    "hots.coding(testloader, trainset.ordering, trainset.classes, layer_threshold = layer_threshold, training=False, ts_batch_size = ts_batch_size, verbose=False, device = device)\n",
    "\n",
    "train_path = f'../Records/output/train/{hots.name}_{num_sample_train}_{jitter}/'\n",
    "test_path = f'../Records/output/test/{hots.name}_{num_sample_test}_{jitter}/'\n",
    "\n",
    "trainset_output = HOTS_Dataset(train_path, trainset.sensor_size, trainset.classes, dtype=trainset.dtype, transform=tonic.transforms.Compose([type_transform]))\n",
    "trainoutputloader = get_loader(trainset_output)\n",
    "testset_output = HOTS_Dataset(test_path, testset.sensor_size, testset.classes, dtype=testset.dtype, transform=type_transform)\n",
    "testoutputloader = get_loader(testset_output)\n",
    "\n",
    "print(f'number of samples in the training set: {len(trainoutputloader)}')\n",
    "print(f'number of samples in the testing set: {len(testoutputloader)}')\n",
    "\n",
    "score = make_histogram_classification(trainset_output, testset_output, N_output_neurons)\n",
    "print(f'Histogram classification performance: {np.round(score,3)*100} %')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15cc5ecf-8251-44d3-9e54-837f26c9816a",
   "metadata": {},
   "source": [
    "tau = 5e3*2\n",
    "events, target = next(iter(trainloader))\n",
    "file_name = f'DVS128_{target}_{tau}_{events.shape[1]}'\n",
    "file_name = f'DVS128_tensor([6])_58068'\n",
    "make_and_display_ts(events.squeeze(0), file_name, trainset, tau, polarity= 'off', nb_frames = 100, ts_batch_size = int(1e4), device = 'cuda')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1912a49-9811-4440-b8fe-0c501facc890",
   "metadata": {},
   "source": [
    "make_and_display_ts(events.squeeze(0), file_name, trainset, tau, polarity= 'on', nb_frames = 100, ts_batch_size = int(1e4), device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead20c9-9f56-48d3-80fa-138152f772e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "learning_rate = 0.00005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 5 + 1\n",
    "sensor_size = (trainset.sensor_size[0],trainset.sensor_size[1],N_output_neurons)\n",
    "tau_cla = 5e3*N_neuronz[-1]\n",
    "drop_proba = .95\n",
    "\n",
    "ts_size = (31,31)\n",
    "ts_batch_size = int(1e4)\n",
    "\n",
    "model_path = f'../Records/networks/{hots.name}_conv_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{drop_proba}_{jitter}.pkl'\n",
    "results_path = f'../Records/LR_results/{hots.name}_conv_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{drop_proba}_{jitter}.pkl'\n",
    "print(model_path)\n",
    "\n",
    "drop_transform = tonic.transforms.DropEvent(p = drop_proba)\n",
    "kfold_mlr = None\n",
    "\n",
    "trainoutputloader = get_loader(trainset_output, kfold = kfold_mlr)\n",
    "\n",
    "classif_layer, losses = fit_mlr(trainoutputloader, model_path, tau_cla, learning_rate, betas, num_epochs, sensor_size, trainset.ordering, len(trainset.classes), ts_size = ts_size, ts_batch_size = ts_batch_size, drop_proba = drop_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601f1d4-cdf6-4ec5-bcfb-f81a0e2daffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe4795e-9b19-4f4f-aee4-19ef6dd05acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_threshold = None\n",
    "ts_batch_size = int(1e3)\n",
    "onlinac, best_probability, meanac, lastac = online_accuracy(classif_layer, tau_cla, testoutputloader, results_path, sensor_size, testset_output.ordering, n_classes, ts_size = ts_size, mlr_threshold = mlr_threshold, ts_batch_size = ts_batch_size, save_likelihood = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084368e-ff26-46d6-90fe-1f0a943b4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(onlinac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20898ac3-3409-4b56-a56a-a39825acb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_threshold = .9\n",
    "ts_batch_size = int(1e3)\n",
    "onlinac, best_probability, meanac, lastac = online_accuracy(classif_layer, tau_cla, testoutputloader, results_path, sensor_size, testset_output.ordering, n_classes, ts_size = ts_size, mlr_threshold = mlr_threshold, ts_batch_size = ts_batch_size, save_likelihood = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c69035-80a6-4eb2-a3bc-65737fbb7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 128, 20\n",
    "print(a//b)\n",
    "print(torch.div(a, b, rounding_mode='floor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb0bf8-7ddf-4513-922d-a5abb1df439a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
