{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1323e1fd-cd1d-4b6f-8de6-0a71f2d7a0b8",
   "metadata": {},
   "source": [
    "# [hotsline](https://github.com/AntoineGrimaldi/hotsline) algorithm to replicate results from [this paper](https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1)\n",
    "## Load events of the NMNIST dataset with [Tonic](https://tonic.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e167bc-5794-4989-8b83-eb671436db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antoine/homhots/hotsline/hots\n",
      " Tonic version installed -> 1.0.19\n",
      "number of samples in the dataset: 1077\n"
     ]
    }
   ],
   "source": [
    "import tonic, torch, os\n",
    "%cd ../hots\n",
    "from utils import get_loader, get_sliced_loader, get_dataset_info, HOTS_Dataset, make_histogram_classification, fit_mlr, predict_mlr, score_classif_events\n",
    "from network import network\n",
    "from timesurface import timesurface\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f' Tonic version installed -> {tonic.__version__}')\n",
    "\n",
    "transform = tonic.transforms.NumpyAsType(int)\n",
    "dataset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True, transform=transform)\n",
    "#get_dataset_info(dataset, properties = ['time', 'mean_isi', 'nb_events']);\n",
    "print(f'number of samples in the dataset: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9ffb72-8ff3-4daf-a619-00e8c9208674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data/DVSGesture/metadata/gesture_1000_True_True\n",
      "Read metadata from disk.\n",
      "../../Data/DVSGesture/metadata/gesture_1000_True_True\n",
      "Read metadata from disk.\n",
      "../../Data/DVSGesture/metadata/gesture_1000_True_False\n",
      "Read metadata from disk.\n",
      "number of samples in the training set: 99\n",
      "number of samples in the testing set: 22\n",
      "['hand_clapping', 'right_hand_wave', 'left_hand_wave', 'right_arm_clockwise', 'right_arm_counter_clockwise', 'left_arm_clockwise', 'left_arm_counter_clockwise', 'arm_roll', 'air_drums', 'air_guitar', 'other_gestures']\n",
      "Number of samples in the trainset set: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                         | 0/22 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.91 GiB (GPU 0; 10.76 GiB total capacity; 3.96 GiB already allocated; 795.50 MiB free; 3.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m classif_layer, losses \u001b[38;5;241m=\u001b[39m fit_mlr(trainoutputloader, model_path, tau_cla, learning_rate, betas, num_epochs, ts_size, trainset\u001b[38;5;241m.\u001b[39mordering, \u001b[38;5;28mlen\u001b[39m(trainset\u001b[38;5;241m.\u001b[39mclasses), ts_batch_size \u001b[38;5;241m=\u001b[39m ts_batch_size)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#plt.plot(losses)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m likelihood, true_target, timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_mlr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassif_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtau_cla\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestoutputloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresults_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mts_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestset_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mts_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mts_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#for i in range(len(likelihood)):\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#    for kernel in range(likelihood[i].shape[1]):\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#        plt.plot(likelihood[i][:,kernel])\u001b[39;00m\n\u001b[1;32m     81\u001b[0m meanac, onlinac, lastac \u001b[38;5;241m=\u001b[39m score_classif_events(likelihood, true_target, n_classes, original_accuracy \u001b[38;5;241m=\u001b[39m score, original_accuracy_nohomeo \u001b[38;5;241m=\u001b[39m score_nohomeo)\u001b[38;5;66;03m#, figure_name = 'nmnist_online.pdf')\u001b[39;00m\n",
      "File \u001b[0;32m~/homhots/hotsline/hots/utils.py:407\u001b[0m, in \u001b[0;36mpredict_mlr\u001b[0;34m(mlrlayer, tau_cla, loader, results_path, timesurface_size, ordering, ts_batch_size)\u001b[0m\n\u001b[1;32m    405\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m load_nb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_batch):\n\u001b[0;32m--> 407\u001b[0m     X, ind_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mtimesurface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesurface_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesurface_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesurface_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau_cla\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mts_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_nb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_timesurface\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprevious_timesurface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    409\u001b[0m         previous_timesurface \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/homhots/hotsline/hots/timesurface.py:68\u001b[0m, in \u001b[0;36mtimesurface\u001b[0;34m(events, sensor_size, ordering, surface_dimensions, tau, decay, filtering_threshold, ts_batch_size, load_number, previous_timesurface, device, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filtering_threshold:\n\u001b[1;32m     67\u001b[0m     indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(all_surfaces\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m>\u001b[39mfiltering_threshold)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     all_surfaces \u001b[38;5;241m=\u001b[39m \u001b[43mall_surfaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_surfaces, indices\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.91 GiB (GPU 0; 10.76 GiB total capacity; 3.96 GiB already allocated; 795.50 MiB free; 3.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "kfold_test = 10\n",
    "kfold_clust = 10\n",
    "ts_batch_size = 2000\n",
    "\n",
    "dataset_name = 'gesture'\n",
    "slicing_time_window = 1e6\n",
    "\n",
    "type_transform = tonic.transforms.NumpyAsType(int)\n",
    "trainset = tonic.datasets.DVSGesture(save_to='../../Data/', train=True, transform=type_transform)\n",
    "testset = tonic.datasets.DVSGesture(save_to='../../Data/', train=False, transform=type_transform)\n",
    "loader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_clust)\n",
    "trainloader = get_sliced_loader(trainset, slicing_time_window, dataset_name, True, only_first=True, kfold=kfold_test)\n",
    "num_sample_train = len(trainloader)\n",
    "testloader = get_sliced_loader(testset, slicing_time_window, dataset_name, False, only_first=True, kfold=kfold_test)\n",
    "num_sample_test = len(testloader)\n",
    "n_classes = len(testset.classes)\n",
    "print(f'number of samples in the training set: {len(trainloader)}')\n",
    "print(f'number of samples in the testing set: {len(testloader)}')\n",
    "\n",
    "name = 'homeohots'\n",
    "homeo = True\n",
    "timestr = '2022-04-22'\n",
    "\n",
    "Rz = [4, 8]\n",
    "N_neuronz = [16, 32]\n",
    "tauz = [5e4*2, 5e4*16]\n",
    "\n",
    "hots = network(name, dataset_name, timestr, trainset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo)\n",
    "\n",
    "#initial_name = hots.name\n",
    "\n",
    "filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "if not os.path.exists('../Records/'):\n",
    "    os.mkdir('../Records/')\n",
    "    os.mkdir('../Records/networks/')\n",
    "path = '../Records/networks/'+hots.name+'.pkl'\n",
    "if not os.path.exists(path):\n",
    "    hots.clustering(loader, trainset.ordering, filtering_threshold = filtering_threshold)\n",
    "    \n",
    "jitter = (None, None)\n",
    "num_workers = 0\n",
    "learning_rate = 0.0005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 8#2 ** 5 + 1\n",
    "N_output_neurons = N_neuronz[-1]\n",
    "ts_size = (trainset.sensor_size[0],trainset.sensor_size[1],N_output_neurons)\n",
    "tau_cla_list = [5e4, 7e4]#, 1e5, 2e5, 3e5, 5e5, 1e6]\n",
    "\n",
    "train_path = f'../Records/output/train/{hots.name}_{num_sample_train}_{jitter}/'\n",
    "test_path = f'../Records/output/test/{hots.name}_{num_sample_test}_{jitter}/'\n",
    "\n",
    "hots.coding(trainloader, trainset.ordering, trainset.classes, training=True, verbose=False)\n",
    "hots.coding(testloader, testset.ordering, testset.classes, training=False, verbose=False)\n",
    "\n",
    "drop_proba = .5\n",
    "drop_transform = tonic.transforms.DropEvent(p = drop_proba)\n",
    "\n",
    "trainset_output = HOTS_Dataset(train_path, trainset.sensor_size, trainset.classes, dtype=trainset.dtype, transform=tonic.transforms.Compose([drop_transform, type_transform]))\n",
    "trainoutputloader = get_loader(trainset_output)\n",
    "testset_output = HOTS_Dataset(test_path, testset.sensor_size, testset.classes, dtype=testset.dtype, transform=tonic.transforms.Compose([drop_transform, type_transform]))\n",
    "testoutputloader = get_loader(testset_output)\n",
    "\n",
    "score = make_histogram_classification(trainset_output, testset_output, N_neuronz[-1])\n",
    "score_nohomeo = 0\n",
    "\n",
    "print(testset_output.classes)\n",
    "\n",
    "for tau_cla in tau_cla_list:\n",
    "\n",
    "    model_path = f'../Records/networks/{hots.name}_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{jitter}.pkl'\n",
    "    results_path = f'../Records/LR_results/{hots.name}_{tau_cla}_{learning_rate}_{betas}_{num_epochs}_{jitter}.pkl'\n",
    "    print(f'Number of samples in the trainset set: {len(trainoutputloader)}')\n",
    "    \n",
    "    classif_layer, losses = fit_mlr(trainoutputloader, model_path, tau_cla, learning_rate, betas, num_epochs, ts_size, trainset.ordering, len(trainset.classes), ts_batch_size = ts_batch_size)\n",
    "    #plt.plot(losses)\n",
    "    likelihood, true_target, timestamps = predict_mlr(classif_layer,tau_cla,testoutputloader,results_path,ts_size,testset_output.ordering,  ts_batch_size = ts_batch_size)\n",
    "    #for i in range(len(likelihood)):\n",
    "    #    for kernel in range(likelihood[i].shape[1]):\n",
    "    #        plt.plot(likelihood[i][:,kernel])\n",
    "    meanac, onlinac, lastac = score_classif_events(likelihood, true_target, n_classes, original_accuracy = score, original_accuracy_nohomeo = score_nohomeo)#, figure_name = 'nmnist_online.pdf')\n",
    "    print(f'For tau = {tau_cla} last accuracy: {lastac*100}% - mean accuracy: {meanac*100}%')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db9f61d3-0cb8-4afa-ada4-40747ae495d8",
   "metadata": {},
   "source": [
    "hots.plotlayers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4950206-7429-4e86-8138-6f112f591156",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = classif_layer.linear.weight.data.cpu().numpy()\n",
    "fig, ax = plt.subplots(N_output_neurons, kernels.shape[0], figsize=(30, 90))\n",
    "for n in range(kernels.shape[0]):\n",
    "    kernel = kernels[n].reshape(trainset.sensor_size[0],trainset.sensor_size[1], N_output_neurons)\n",
    "    for p in range(N_output_neurons):\n",
    "        ax[p, n].imshow(kernel[:,:,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6ef67-582c-473d-be02-a93885437f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
