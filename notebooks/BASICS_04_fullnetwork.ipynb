{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146e9eb1-7f15-4122-a866-cd3061b73da0",
   "metadata": {},
   "source": [
    "# BASICS 04 - HOTS network with online classification layer\n",
    "\n",
    "## TODO:\n",
    "- check is the loss decreases significantly during training for the whole training set or if it's possible to train only on a subset\n",
    "- make a network with all the layers included \n",
    "    - clustering first and then coding with training of the mlr with the network (saving the output event stream serves only to train the MLR afterward, can be done in only 1 pass?)\n",
    "    - have a plot of the different losses and entropy of the TS learnt\n",
    "    - have a plot of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a75ff8-780a-400b-9925-515f4a2274cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6346880-ad3c-4a2c-a39a-1a8842c3ac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/antoine/homhots/hotsline/hots\n",
      "Tonic version installed -> 1.0.15\n",
      "Number of GPU devices available: 1\n",
      "GPU 1 named GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "%cd ../hots\n",
    "import tonic, torch, os\n",
    "from timesurface import timesurface\n",
    "from network import network\n",
    "from utils import get_loader\n",
    "\n",
    "print(f'Tonic version installed -> {tonic.__version__}')\n",
    "\n",
    "print(f'Number of GPU devices available: {torch.cuda.device_count()}')\n",
    "for N_gpu in range(torch.cuda.device_count()):\n",
    "    print(f'GPU {N_gpu+1} named {torch.cuda.get_device_name(N_gpu)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce771bc-ad2d-45c4-ba80-0a28862f07fe",
   "metadata": {},
   "source": [
    "## Initialization of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1caf366e-fc63-4030-868b-a57babe598de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m N_neuronz \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      8\u001b[0m tauz \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5e3\u001b[39m, \u001b[38;5;241m5e4\u001b[39m, \u001b[38;5;241m5e5\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m network \u001b[38;5;241m=\u001b[39m network(name, dataset_name, timestr, \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39msensor_size, nb_neurons \u001b[38;5;241m=\u001b[39m N_neuronz, tau \u001b[38;5;241m=\u001b[39m tauz, R \u001b[38;5;241m=\u001b[39m Rz, homeo \u001b[38;5;241m=\u001b[39m homeo)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "name = 'homeohots'\n",
    "homeo = True\n",
    "timestr = '2022-04-01'\n",
    "dataset_name = 'nmnist'\n",
    "\n",
    "Rz = (2, 4, 8)\n",
    "N_neuronz = (4, 8, 16)\n",
    "tauz = (5e3, 5e4, 5e5)\n",
    "\n",
    "network = network(name, dataset_name, timestr, dataset.sensor_size, nb_neurons = N_neuronz, tau = tauz, R = Rz, homeo = homeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ace1c-cf49-494e-84a5-733ef062cddc",
   "metadata": {},
   "source": [
    "## Loading of the dataset for the clustering phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbdeff-9a63-4aed-a950-073d75af4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = True\n",
    "transform = tonic.transforms.NumpyAsType(int)\n",
    "dataset = tonic.datasets.NMNIST(save_to='../../Data/', train=trainset, transform=transform)\n",
    "loader = get_loader(dataset, kfold = 300, shuffle=True)\n",
    "#get_dataset_info(dataset, properties = ['time', 'mean_isi', 'nb_events']);\n",
    "print(f'number of samples in the dataset: {len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804365ce-3c09-40a8-90ad-f2471961b169",
   "metadata": {},
   "source": [
    "##Â Unsupervised clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7064b789-7876-4611-8d34-3a5a4d62e888",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'network' has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Records/networks/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m filtering_threshold \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mRz[L] \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Rz))]\n\u001b[0;32m----> 5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Records/networks/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m      7\u001b[0m     network\u001b[38;5;241m.\u001b[39mclustering(loader, dataset\u001b[38;5;241m.\u001b[39mordering, filtering_threshold)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'network' has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../Records/'):\n",
    "    os.mkdir('../Records/')\n",
    "    os.mkdir('../Records/networks/')\n",
    "filtering_threshold = [2*Rz[L] for L in range(len(Rz))]\n",
    "path = '../Records/networks/'+network.name+'.pkl'\n",
    "if not os.path.exists(path):\n",
    "    network.clustering(loader, dataset.ordering, filtering_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0361ae-8651-4ca6-95db-d3161988f90c",
   "metadata": {},
   "source": [
    "## Coding of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafb38e7-28d4-4efc-8375-7cb1a8fae24f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m get_loader(\u001b[43mdataset\u001b[49m, kfold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m network\u001b[38;5;241m.\u001b[39mcoding(loader, dataset\u001b[38;5;241m.\u001b[39mordering, dataset\u001b[38;5;241m.\u001b[39mclasses, filtering_threshold, training\u001b[38;5;241m=\u001b[39mtrainset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "loader = get_loader(dataset, kfold = None, shuffle=True)\n",
    "network.coding(loader, dataset.ordering, dataset.classes, filtering_threshold, training=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3ff6e-d596-4889-b65b-ebbd60eb23c3",
   "metadata": {},
   "source": [
    "## Coding of the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60152ea5-3bb9-45b2-ab2a-5cff12e353db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tonic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtonic\u001b[49m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mNMNIST(save_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../Data/\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39mtrainset, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m get_loader(dataset, kfold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m network\u001b[38;5;241m.\u001b[39mcoding(loader, dataset\u001b[38;5;241m.\u001b[39mordering, dataset\u001b[38;5;241m.\u001b[39mclasses, filtering_threshold, training\u001b[38;5;241m=\u001b[39mtrainset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tonic' is not defined"
     ]
    }
   ],
   "source": [
    "trainset = False\n",
    "dataset = tonic.datasets.NMNIST(save_to='../../Data/', train=trainset, transform=transform)\n",
    "loader = get_loader(dataset, kfold = None, shuffle=True)\n",
    "network.coding(loader, dataset.ordering, dataset.classes, filtering_threshold, training=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd54d0-750d-46b0-9406-841921037639",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "- add plotlayer from neuronhots\n",
    "- add plot of the loss\n",
    "- add the MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de8a7e-4613-48a8-81a4-6065c1c84b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
